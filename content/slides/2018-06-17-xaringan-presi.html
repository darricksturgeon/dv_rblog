---
title: Visualizing Clusters
author: Darrick Sturgeon
slug: xaringan-presi
categories: []
tags: []
description: Desc
hacker_news_id: ''
lobsters_id: ''
meta_img: /images/image.jpg
output: xaringan::moon_reader
---



<p>class: inverse # The Dataset</p>
<p>I use the Iris dataset for this exercise.</p>
<div class="figure">
<img src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png" alt="Iris Dataset" />
<p class="caption">Iris Dataset</p>
</div>
<p>This dataset is a very nice dataset for clustering problems for reasons we’ll address next.</p>
<hr />
<p><img src="/slides/2018-06-17-xaringan-presi_files/figure-html/unnamed-chunk-2-1.png" width="1008" /></p>
<p>class: inverse # Clustering</p>
<p>Generally, clustering falls under the purvue of unsupervised machine learning. We want to see what ‘natural’ boundaries we can find to divide a dataset without telling it anything explicit about the classes we are looking for. This could be to validate the classes, or, to find new classes if we have no prior information.</p>
<div id="gaussian-mixture-models" class="section level2">
<h2>Gaussian Mixture Models</h2>
<p>Probably the most prolific clustering method is “K-means”, which attempts to split the data into K distinct centroids (means) by minimizing variance. Classes are determined by which center a given data point is closest to.</p>
<p>In contrast, a Gaussian Mixture Model gives both class mean as well as variances under a multivariate normal distribution. In some ways, the GMM is a natural extension of Kmeans to a probabilistic model.</p>
<p><a href="https://upload.wikimedia.org/wikipedia/commons/8/8e/MultivariateNormal.png">Multivariate Normal</a></p>
<p>class: inverse # Plotting the Results</p>
<pre class="r"><code>plt &lt;- ggplot(comps, aes(x = pc1, y = pc2)) +
  geom_point(data = comps[comps$correct==FALSE,], mapping = aes(color = correct), size = 6, solid = F, shape = 1) +
  geom_point(aes(color = cls), size = 2) +
  stat_ellipse(aes(color = cls), type = &quot;norm&quot;, level = .68, show.legend = FALSE) +
  stat_ellipse(aes(color = cls), type = &quot;norm&quot;, linetype = 2, level = .95, show.legend = FALSE) +
  scale_color_manual(values = c(&#39;#4daf4a&#39;,&#39;#377eb8&#39;,&#39;#a65628&#39;,&#39;#e41a1c&#39;),
                     labels = c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;misclassified&#39;),
                     name = &quot;GMM Class&quot;) +
  ggtitle(&#39;The Iris Dataset&#39;, subtitle = &#39;Unsupervised Clustering using a GMM&#39;) +
  xlab(&#39;Principal Component 1&#39;) +
  ylab(&#39;Principal Component 2&#39;) +
  guides(color = guide_legend(override.aes = list(linetype = c(1, 1, 1, 0), shape = c(16, 16, 16, 1), size = c(2, 2, 2, 3)))) +
  theme(text=element_text(size=16, family=&quot;Lato&quot;)) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2, 3))</code></pre>
<p>class: inverse # How to Read and what to Look for</p>
<p>This is a scatter plot with two things added: Some information about the distributions found by the model, and information about where the model failed to classify points.</p>
<p>While not explicitly stated due to legend difficulties, The solid and dotted lines respectively represent the 1 standard deviation (68%) and 2 standard deviation (97%) levels for that multivariate gaussian.</p>
<p>The Axes shown are the first two principal components - this plot is more suited to someone who is somewhat familiar with PCA. The 4 variable dataset here has been translated, scaled, and rotated to emphasize variance. This generally results in greater class seperability in the first two principal components than in any two variables we might have plotted prior to this transformation. It is not strictly necessary, but it can help for a single visualization.</p>
<p>The other option might be to exhaustively plot the different combinations of the 4 variables in this dataset to get a picture of the distribution.</p>
<p>class: inverse # Making the Plot</p>
<p>In preparation, two data transformations were key here:</p>
<ul>
<li><p>scaling and rotating the data according to the pca transform</p></li>
<li><p>matching “like” class labels to determine misclassifications.</p></li>
</ul>
<p>–</p>
<p>The “stat_ellipse” geom proved extremely useful for this task. Stat ellipse fits a t or normal distribution to a set of data. By fitting a normal distribution to each of the GMM’s classifications, you recover the statistics of the GMM.</p>
<p>class: inverse # Sources</p>
<ul>
<li><p>R data camp: Iris Photos</p></li>
<li><p>wikimedia: Multivariate Gaussian Picture</p></li>
</ul>
</div>
