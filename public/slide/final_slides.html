<!DOCTYPE html>
<html>
  <head>
    <title>Visualizing Clusters with Iris</title>
    <meta charset="utf-8">
    <meta name="author" content="Darrick Sturgeon" />
    <meta name="date" content="2018-06-18" />
    <link href="final_slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="final_slides_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Visualizing Clusters with Iris
### Darrick Sturgeon
### June 18, 2018

---

class: inverse
# The Dataset


I chose the cran built-in iris dataset.

![Iris Dataset](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)

This dataset is a very nice dataset for clustering.

Sorry for choosing an easy dataset!  I had meant to work through the visualization first and repeat this with a more interesting example.

---
class: inverse
# The Data



&lt;img src="final_slides_files/figure-html/unnamed-chunk-2-1.png" width="1008" /&gt;

---
class: inverse
# The Objective

The goal is to explore the data using clustering methods.  To do this, we will employ and visualize a Gaussian Mixture Model, we will also employ Principal Component Analysis to (hopefully) give the best view of the clusters.

GMMs for clustering can be effective in cases where the data:

--

- has or is expected to have groupings, classes, or some sort of separability

--

- consists of several continuous variables

--

- has a comfortably large number of observations

--

- preferably no missing data

---
class: inverse
# Clustering


Generally, clustering falls under the purvue of unsupervised machine learning.  We want to see what 'natural' boundaries we can find to divide a dataset without telling it anything explicit about the classes we are looking for.  This could be to validate the classes, or, to find new classes if we have no prior information.


## Gaussian Mixture Models

Probably the most prolific clustering method is "K-means", which in short, attempts to effectively split the data into several "mean" values, where data is classified based on proximity to a given mean.

In contrast, a Gaussian Mixture Model gives both class mean as well as variances under a multivariate normal distribution.  In some ways, the GMM is a natural extension of Kmeans to a probabilistic model, where we can now say something about the expected distribution of new data.

[Multivariate Normal](https://upload.wikimedia.org/wikipedia/commons/8/8e/MultivariateNormal.png)

---
class: inverse
# The Setup

GMMs can be done in R using the package "mixtools".  Unfortunately, the package is not conformed to the tidyverse so you end up with some nested lists as outputs instead of dataframes.  There is also a package "mclust", which is possibly a better choice in terms of features or organization (I haven't had the chance to try it).

One important aspect of machine learning are training and testing data sets, so we randomly split the data into training data (which we will use the statistical model on) and testing data (which we will use to validate the model's effectiveness).

So I trained on a random 60% of the data, then classified the remaining test data using the result, and recombined data into a dataframe, marking where the classes mismatched.


---
class: inverse
# Making the Plot

- The "stat_ellipse" geom proved extremely useful for this task.  Stat ellipse fits a t or normal distribution to a set of data.  By fitting a normal distribution to each of the GMM's classifications, you recover the covariances of those components in the GMM.

- To employ the facet wrap in the way I wanted, I had to create a second dataframe with the facet renamed, that way it will not get wrapped along with the rest.  Effectively, the ellipses drawn for the GMM need to be based only on the training data, not on the testing data.

- The colors took a little bit, I wanted two colors which contrast very starkly, (blue and brown) but also such that red still draws the eye enough.

-




```
number of iterations= 32 
```






```r
pca &lt;- prcomp(data, scale=TRUE, center=TRUE)
comps &lt;- data.frame(pca$x)
colnames(comps) &lt;- c('pc1', 'pc2', 'pc3', 'pc4')
comps$Species &lt;- iris$Species
comps &lt;- rbind(comps[idx,], comps[-idx,])
```


```r
tmp &lt;- as.data.frame(mvn$posterior)  %&gt;% 
  mutate(cls=apply(.[,], 1, function(x) names(x)[which.max(x)])) %&gt;% 
  select(cls) %&gt;%
  mutate(train = 'TRAIN')

tmp2 &lt;- t(classify(as.matrix(test)))
colnames(tmp2) &lt;- c('comp.1', 'comp.2', 'comp.3')
tmp2 &lt;- as.data.frame(tmp2) %&gt;%
  mutate(cls=apply(.[,], 1, function(x) names(x)[which.max(x)])) %&gt;% 
  select(cls) %&gt;%
  mutate(train = 'TEST')
```


```r
tm &lt;- rbind(tmp, tmp2)
comps$cls &lt;- tm$cls
comps$train &lt;- tm$train
comps &lt;- comps %&gt;% mutate(cls = factor(cls))
comps &lt;- comps %&gt;% mutate(correct = case_when(Species == 'setosa' &amp; cls == 'comp.1' ~ TRUE,
                           Species == 'versicolor' &amp; cls == 'comp.2' ~ TRUE,
                           Species == 'virginica' &amp; cls == 'comp.3' ~ TRUE,
                           TRUE ~ FALSE))
iris2 &lt;- rbind(iris[idx,], iris[-idx,])
iris2$cls &lt;- comps$cls
iris2$train &lt;- comps$train
iris2 &lt;- iris2 %&gt;% mutate(cls = factor(cls))
iris2$correct &lt;- comps$correct
```


---
# The Code
class: inverse


```r
not_iris &lt;- iris2

not_iris &lt;- not_iris %&gt;% mutate(ntrain = train)
not_iris &lt;- not_iris[,!(names(not_iris) %in% c('train'))]

iris2 &lt;- iris2 %&gt;% mutate(train = factor(train, levels = c('TRAIN', 'TEST')))

plt &lt;- ggplot(iris2, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(data = iris2[iris2$correct==FALSE,], mapping = aes(color = correct), size = 6, solid = F, shape = 1) +
  geom_point(aes(color = cls, shape = Species), size = 2) +
  stat_ellipse(data = not_iris[not_iris$ntrain == 'TRAIN',], aes(x = Petal.Width, y = Petal.Length, color = cls), type = "norm", level = .68, show.legend = FALSE) +
  stat_ellipse(data = not_iris[not_iris$ntrain == 'TRAIN',], aes(x = Petal.Width, y = Petal.Length, color = cls), type = "norm", linetype = 2, level = .95, show.legend = FALSE) +
  scale_color_manual(values = c('#4daf4a','#377eb8','#a65628','#e41a1c'),
                     labels = c('setosa', 'versicolor', 'virginica', 'misclassified'),
                     name = "Predicted Class") +
  scale_shape_manual(values = c(1, 2, 3), name = 'True Class') +
  ggtitle('The Iris Dataset', subtitle = 'Unsupervised Clustering using a GMM') +
  xlab('Petal Width (cm)') +
  ylab('Petal Length (cm)') +
  guides(color = guide_legend(override.aes = list(linetype = c(1, 1, 1, 0), shape = c(16, 16, 16, 1), size = c(2, 2, 2, 3)))) +
  theme(text=element_text(size=16, family="Lato")) + facet_wrap(~train)

#+
  #scale_x_continuous(breaks = c(-2, -1, 0, 1, 2, 3))
```

---
class: inverse


```r
plt
```

&lt;img src="final_slides_files/figure-html/unnamed-chunk-5-1.png" width="1008" /&gt;


---
class: inverse
# Plot 1.0 code


```r
plt &lt;- ggplot(comps, aes(x = pc1, y = pc2)) +
  geom_point(data = comps[comps$correct==FALSE,], mapping = aes(color = correct), size = 6, solid = F, shape = 1) +
  geom_point(aes(color = cls), size = 2) +
  stat_ellipse(data = comps[comps[,'train']=='TRAIN',], aes(color = cls), type = "norm", level = .68, show.legend = FALSE) +
  stat_ellipse(data = comps[comps[,'train']=='TRAIN',], aes(color = cls), type = "norm", linetype = 2, level = .95, show.legend = FALSE) +
  scale_color_manual(values = c('#4daf4a','#377eb8','#a65628','#e41a1c'),
                     labels = c('setosa', 'versicolor', 'virginica', 'misclassified'),
                     name = "Predicted Class") +
  ggtitle('The Iris Dataset', subtitle = 'Unsupervised Clustering using a GMM') +
  xlab('Principal Component 1') +
  ylab('Principal Component 2') +
  guides(color = guide_legend(override.aes = list(linetype = c(1, 1, 1, 0), shape = c(16, 16, 16, 1), size = c(2, 2, 2, 3)))) +
  theme(text=element_text(size=16, family="Lato")) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2, 3)) + facet_wrap(~train)
```


---
class: inverse
# Plot 1.0

&lt;img src="final_slides_files/figure-html/unnamed-chunk-6-1.png" width="1008" /&gt;


---
class: inverse
# How to Read It and What to Look For

This is a scatter plot with two things added:  Some information about the distributions found by the model, which are multivariate gaussians, and information about the classification encoded into shape and color.

While not explicitly stated due to legend difficulties, The solid and dotted lines respectively represent the 1 standard deviation (68%) and 2 standard deviation (97%) levels for the respective multivariate gaussian.

The true class is represented by shape, and the predicted class is represented by the color, which matches the color of the associated gaussian.  The red circles help draw the eye two where a shape/color mismatch has occurred.





---
class: inverse
# How to make it even better


You can actually compute the curves where the gaussian level sets intersect, meaning we could show the class decision boundaries.  These are very cool plots but obviously take a bit more time to set up.

There are two annotations I would add:  accuracy to both plots, and either a legend or annotations for the gaussian level sets.

I still don't know the best way to visualize true classes vs. predicted classes, I thought this did a fairly good job, but if the number of clusters increases by much...  6 different shapes may get horrendous.  In the end, I think maybe three plots may do the trick:  True class, predicted class, highlighted misclassifications (everything else grey).  These in succession would be extremely clear.



---
class: inverse

# Issues/Observations

Adding 'linestyle' to the legend proved rather difficult, as the two types of line plotted: solid and dashed, were really not based on any facet of the data.  This proves quite difficult for ggplot, where most suggestions are to simply hack the plot to have such a distinction, e.g., adding a geom_line with alpha=0.  I tried this and it did not work as intended.

--

ggplot doesn't offer anything too nice for geometric shapes.  The "stat_ellipse" function allows you to fit a normal distribution to a set of data and draw the level curves around it.  This is actually equivalent to the solution for the GMM if you "color" by the classes output from the GMM.  If, for some reason, stat_ellipse didn't give the precise ellipses you were seeking, or you wish to plot some other curve, you would have to plot the two (upper and lower) curves for each ellipse.  Basically, a lot of algebra for the average user.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
